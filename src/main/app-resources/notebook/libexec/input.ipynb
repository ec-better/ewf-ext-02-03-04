{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewf-ext-02-03-04 - NDVI anomalies of growing season time series per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NDVI anomalies of growing season time series per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'NDVI anomalies of growing season time series per pixel'),\n",
    "                ('abstract', 'NDVI anomalies of growing season time series per pixel'),\n",
    "                ('id', 'ewf-ext-02-03-04')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parameter\">Parameter Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionOfInterest = dict([('id', 'regionOfInterest'),\n",
    "                         ('value', 'POLYGON ((-8.864205 38.88616500000001, -8.864205 38.986165, -8.964205000000002 38.986165, -8.964205000000002 38.88616500000001, -8.864205 38.88616500000001))'),\n",
    "                         ('title', 'WKT Polygon for the Region of Interest'),\n",
    "                         ('abstract', 'Set the value of WKT Polygon')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameOfRegion = dict([('id', 'nameOfRegion'),\n",
    "                     ('value', 'P001'),\n",
    "                     ('title', 'Name of Region'),\n",
    "                     ('abstract', 'Name of the region of interest'),\n",
    "                     ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexAndApiKeys = dict([('id', 'indexAndApiKeys'),\n",
    "                        ('value', ''),\n",
    "                        ('title', 'index,apikey pairs'),\n",
    "                        ('abstract', 'index,apikey pairs'),\n",
    "                        ('minOccurs', '1')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "Agg and LTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_identifiers = ('60C6662B91CEB188007C3809F852EC0FB0378E2E','1D976D213B24ABFA2CAA46870019E296CE47A06E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "Agg and LTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/better-ext-02-03-02/search?format=atom&uid=60C6662B91CEB188007C3809F852EC0FB0378E2E', 'https://catalog.terradue.com/better-ext-02-03-03/search?format=atom&uid=1D976D213B24ABFA2CAA46870019E296CE47A06E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal, ogr, osr\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import cioppy\n",
    "ciop = cioppy.Cioppy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_results = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata from catalog\n",
    "def get_input_metadata (input_refs, apikeys):\n",
    "    \n",
    "    # for each product get metadata\n",
    "    Result_Prod = []\n",
    "    \n",
    "    for index,product_ref in enumerate(input_refs):\n",
    "        \n",
    "        for index in apikeys:\n",
    "            if index in product_ref:\n",
    "                cat_index = index\n",
    "                cat_apikey = apikeys[index]\n",
    "        \n",
    "        # since the search is by identifier\n",
    "        Result_Prod.append(ciop.search(end_point = product_ref,params =[],output_fields='self,identifier,startdate,enclosure,startdate,enddate,wkt,title',creds='{}:{}'.format(cat_index,cat_apikey))[0] )\n",
    "    \n",
    "\n",
    "    input_metadata = gpd.GeoDataFrame.from_dict(Result_Prod)\n",
    "\n",
    "    input_metadata['startdate'] = pd.to_datetime(input_metadata['startdate'])\n",
    "    input_metadata['enddate'] = pd.to_datetime(input_metadata['enddate'])\n",
    "    \n",
    "    return input_metadata\n",
    "\n",
    "    \n",
    "def get_metadata(filepath):\n",
    "\n",
    "    ds = gdal.Open(filepath)\n",
    "    projection = ds.GetProjection()\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    no_data_value = ds.GetRasterBand(1).GetNoDataValue()\n",
    "    data_type = ds.GetRasterBand(1).DataType\n",
    "    return projection, geotransform, no_data_value, data_type\n",
    "\n",
    "\n",
    "def get_matrix_list(image_list):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "        \n",
    "        print(type(product_array))\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "def get_matrix_list(image_list, mask_value = None):\n",
    "    mat_list = []\n",
    "    for img in image_list:\n",
    "        dataset = gdal.Open(img)\n",
    "        product_array = dataset.GetRasterBand(1).ReadAsArray()\n",
    "        \n",
    "        if mask_value is not None:\n",
    "             product_array = np.ma.masked_values (product_array, mask_value)\n",
    "        \n",
    "        mat_list.append(product_array)\n",
    "        dataset = None\n",
    "    return mat_list\n",
    "\n",
    "\n",
    "\n",
    "def calc_anomaly(agg_file, LTA_file):\n",
    "    \n",
    "    \n",
    "    if agg_file and LTA_file:\n",
    "        \n",
    "        \n",
    "        agg_and_LTA = get_matrix_list([agg_file, LTA_file], -9999.)\n",
    "        print('Aggregation and LTA converted to matrices')\n",
    "                        \n",
    "        anomaly_values = np.divide(agg_and_LTA[0] * 1.0, agg_and_LTA[1] * 1.0, out=np.zeros(agg_and_LTA[0].shape) - 9999.0, where=np.logical_and(agg_and_LTA[1]!=0, agg_and_LTA[0] > -9998, agg_and_LTA[1] > -9998) )\n",
    "        \n",
    "        anomaly_values = np.ma.masked_values (anomaly_values, -9999.0)\n",
    "        \n",
    "        \n",
    "        projection, geotransform, no_data_value, data_type = get_metadata(agg_file)\n",
    "                \n",
    "        return anomaly_values, projection, geotransform, no_data_value, data_type\n",
    "    \n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def write_output_image(filepath, output_matrix, image_format, data_format, mask=None, output_projection=None, output_geotransform=None, no_data_value=None):\n",
    "    driver = gdal.GetDriverByName(image_format)\n",
    "    out_rows = np.size(output_matrix, 0)\n",
    "    out_columns = np.size(output_matrix, 1)\n",
    "    if mask is not None and mask is not 0:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 2, data_format)\n",
    "        mask_band = output.GetRasterBand(2)\n",
    "        mask_band.WriteArray(mask)\n",
    "        if no_data_value is not None:\n",
    "            output_matrix[mask > 0] = no_data_value\n",
    "    else:\n",
    "        output = driver.Create(filepath, out_columns, out_rows, 1, data_format)\n",
    "    \n",
    "    if output_projection is not None:\n",
    "        output.SetProjection(output_projection)\n",
    "    if output_geotransform is not None:\n",
    "        output.SetGeoTransform(output_geotransform)\n",
    "    \n",
    "    raster_band = output.GetRasterBand(1)\n",
    "    if no_data_value is not None:\n",
    "        raster_band.SetNoDataValue(no_data_value)\n",
    "    raster_band.WriteArray(output_matrix)\n",
    "    gdal.Warp(filepath, output, format=\"GTiff\", outputBoundsSRS='EPSG:4326', xRes=output_geotransform[1], yRes=-output_geotransform[5], targetAlignedPixels=True)\n",
    "\n",
    "    \n",
    "\n",
    "def write_anomaly_output(anomaly, output_folder, mission_name, stats_name, first_date, last_date, lta_start_year, lta_end_year, mask_no_value, regionOfInterest, roi_name, projection, geo_transform, no_data_value):\n",
    "    \n",
    "    stats_period = first_date.strftime('%Y%j') + '_' + last_date.strftime('%Y%j')\n",
    "    LTA_period = str(lta_start_year) + '-' + str(lta_end_year)\n",
    "    \n",
    "    filename = '_'.join(['Anomaly', mission_name, stats_name, roi_name, stats_period, 'LTA', LTA_period])\n",
    "    \n",
    "    filename = os.path.join(output_folder, filename + '.tif')\n",
    "    \n",
    "    write_output_image(filename, anomaly, 'GTiff', gdal.GDT_Float32, mask_no_value, projection, geo_transform, no_data_value)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_formatted_date(date_str):\n",
    "    date = datetime.datetime.strftime(date_str, '%Y-%m-%dT00:00:00Z')\n",
    "    return date\n",
    "\n",
    "\n",
    "def write_properties_file(output_name, first_date, last_date, region_of_interest):\n",
    "    \n",
    "    title = 'Output %s' % output_name\n",
    "    \n",
    "    first_date = get_formatted_date(first_date)\n",
    "    last_date = get_formatted_date(last_date)\n",
    "    \n",
    "    with open(output_name + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (first_date, last_date))\n",
    "        file.write('geometry=%s' % (region_of_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(output_folder) > 0:\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'Getting metadata from catalog' \n",
    "ciop.log('INFO', message)\n",
    "\n",
    "\n",
    "# organize indexes and apikeys in a python dictionary\n",
    "indexAndApiKeys_splited = indexAndApiKeys['value'].split(',')\n",
    "apikeys = {}\n",
    "for idx,ele in enumerate(indexAndApiKeys_splited):\n",
    "    \n",
    "    if (idx % 2 == 0):\n",
    "        print(ele)\n",
    "        apikeys[ele] = indexAndApiKeys_splited[idx+1]\n",
    "        \n",
    "\n",
    "# get input data from catalog\n",
    "input_metadata = get_input_metadata (input_references, apikeys)\n",
    "\n",
    "\n",
    "input_metadata_LTA = input_metadata[input_metadata['title'].str.find('LTA') != -1]\n",
    "input_metadata_Agg = input_metadata[input_metadata['title'].str.find('LTA') == -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata_LTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metadata_Agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file paths\n",
    "filepath_agg = os.path.join(data_path, input_metadata_Agg['enclosure'].iloc[0].split('/')[-1])\n",
    "filepath_LTA = os.path.join(data_path, input_metadata_LTA['enclosure'].iloc[0].split('/')[-1])\n",
    "\n",
    "print(filepath_agg,filepath_LTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if isinstance(input_identifiers, str):\n",
    "    input_identifiers = [input_identifiers]\n",
    "\n",
    "region_of_interest = regionOfInterest['value']\n",
    "name_of_region = nameOfRegion['value']\n",
    "\n",
    "\n",
    "# Agg\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_agg).split('.')[0].split('_')\n",
    "misson_name = file_name_elements[0]\n",
    "stats_name = file_name_elements[1]\n",
    "aoi_name = file_name_elements[2]\n",
    "\n",
    "print(misson_name, stats_name, aoi_name)\n",
    "\n",
    "\n",
    "first_date = datetime.datetime.strptime(file_name_elements[3], \"%Y%j\")\n",
    "last_date = datetime.datetime.strptime(file_name_elements[4], \"%Y%j\")\n",
    "\n",
    "print(first_date)\n",
    "print(last_date)\n",
    "\n",
    "\n",
    "# LTA\n",
    "\n",
    "file_name_elements = os.path.basename(filepath_LTA).split('.')[0].split('_')\n",
    "\n",
    "start_year = file_name_elements[5].split('-')[0]\n",
    "end_year = file_name_elements[-1].split('-')[1]\n",
    "\n",
    "print(file_name_elements)\n",
    "\n",
    "stats_name_LTA = file_name_elements[2]\n",
    "\n",
    "\n",
    "print(start_year)\n",
    "print(end_year)\n",
    "\n",
    "print(filepath_agg)\n",
    "print(filepath_LTA)        \n",
    "\n",
    "message = 'Computing Anomaly' \n",
    "ciop.log('INFO', message)\n",
    "    \n",
    "    \n",
    "anomaly_values, projection, geotransform, no_data_value, data_type = calc_anomaly(filepath_agg, filepath_LTA)\n",
    "\n",
    "\n",
    "\n",
    "message = 'Writing anomaly image' \n",
    "ciop.log('INFO', message)\n",
    "\n",
    "filename = write_anomaly_output(anomaly_values.data, output_folder, misson_name, stats_name, first_date, last_date, start_year, end_year, None, region_of_interest, name_of_region, projection, geotransform, no_data_value)\n",
    "\n",
    "print(filename)\n",
    "\n",
    "write_properties_file(filename, first_date, last_date, region_of_interest)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
